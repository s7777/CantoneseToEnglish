{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJxos0Yw09KV",
        "outputId": "23ba701f-3492-4efc-deda-63b93c312e72"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t#\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'yue.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-Cantonese.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Cantonese.pkl\n",
            "[Begin] => [開始啦。]\n",
            "[Hello] => [哈佬。]\n",
            "[Go away] => [死開！]\n",
            "[Go away] => [彈開！]\n",
            "[Get lost] => [死開啦！]\n",
            "[Get lost] => [躝開啦！]\n",
            "[Good job] => [做得好！]\n",
            "[I get it] => [我明。]\n",
            "[I get it] => [我get到。]\n",
            "[Im here] => [我喺度。]\n",
            "[After you] => [你先。]\n",
            "[Dont cry] => [唔好喊啦。]\n",
            "[Im angry] => [我好嬲。]\n",
            "[Im ready] => [我準備好喇。]\n",
            "[Im right] => [我係啱嘅。]\n",
            "[Its easy] => [好容易噃。]\n",
            "[Sit still] => [坐定定。]\n",
            "[Well done] => [做得好！]\n",
            "[Be serious] => [咪玩啦。]\n",
            "[Be serious] => [認真啲啦。]\n",
            "[Come on in] => [入嚟啦！]\n",
            "[Dont move] => [咪郁。]\n",
            "[Hows work] => [返工返成點呀？]\n",
            "[I know him] => [我識佢。]\n",
            "[Is it safe] => [安唔安全㗎？]\n",
            "[It matters] => [重要。]\n",
            "[Its windy] => [好大風。]\n",
            "[Let me die] => [俾我死咗去算啦。]\n",
            "[No problem] => [冇問題。]\n",
            "[Say please] => [講唔該吖。]\n",
            "[Start over] => [重新嚟過啦。]\n",
            "[Take a nap] => [瞌一陣啦。]\n",
            "[Time is up] => [夠鐘喇。]\n",
            "[We are men] => [我哋係男人。]\n",
            "[What is it] => [呢個咩嚟㗎？]\n",
            "[Who did it] => [邊個做架？]\n",
            "[Who talked] => [邊個講嘢？]\n",
            "[Who yelled] => [邊個嗌呀？]\n",
            "[Dont worry] => [唔使擔心。]\n",
            "[He helps us] => [佢幫我哋。]\n",
            "[He stood up] => [佢企咗起身。]\n",
            "[How are you] => [你好嗎？]\n",
            "[I love cake] => [我鐘意蛋糕。]\n",
            "[Please stay] => [請留步。]\n",
            "[Please stay] => [請你留低啦。]\n",
            "[Talk slower] => [講慢少少。]\n",
            "[Abandon ship] => [棄船呀！]\n",
            "[Come with us] => [一齊嚟啦。]\n",
            "[Dont be sad] => [唔好唔開心啦。]\n",
            "[Drive slowly] => [揸慢啲。]\n",
            "[I wont lose] => [我唔會輸嘅！]\n",
            "[Im busy now] => [我而家好唔得閒。]\n",
            "[Im ticklish] => [我好驚擳。]\n",
            "[Its snowing] => [落緊雪。]\n",
            "[Its too big] => [太大喇。]\n",
            "[Just shut up] => [你唔出聲冇人當你啞㗎喎。]\n",
            "[Now drink up] => [乾咗佢！]\n",
            "[Speak slower] => [講慢少少。]\n",
            "[This is food] => [嘢食嚟。]\n",
            "[Tom screamed] => [Tom尖叫。]\n",
            "[What a waste] => [嘥晒！]\n",
            "[Youre right] => [你講得啱。]\n",
            "[Youre right] => [你講得冇錯。]\n",
            "[Are you tired] => [你唔攰咩？]\n",
            "[He has a cold] => [佢冷親。]\n",
            "[He has a cold] => [佢作感冒。]\n",
            "[He has a cold] => [佢傷風。]\n",
            "[I lost my key] => [我唔見咗我條鎖匙。]\n",
            "[I read a book] => [我睇咗一本書。]\n",
            "[I want to cry] => [我想喊。]\n",
            "[Im a tourist] => [我係旅客。]\n",
            "[Im not lucky] => [我好黑仔。]\n",
            "[Im on my way] => [我嚟緊喇。]\n",
            "[Let me see it] => [俾我睇吓。]\n",
            "[No thank you] => [唔使喇，唔該。]\n",
            "[Say something] => [講啲嘢啦。]\n",
            "[Send Tom away] => [送Tom走。]\n",
            "[Take him away] => [帶佢走。]\n",
            "[Tom called me] => [阿Tom打咗電話畀我。]\n",
            "[You all right] => [你冇嘢吖嘛？]\n",
            "[Can you cancel] => [你可唔可以取消？]\n",
            "[Cross the road] => [過馬路啦。]\n",
            "[Do you love me] => [你鍾唔鍾意我㗎？]\n",
            "[Dont run here] => [唔好喺呢度跑嚟跑去。]\n",
            "[Give it a shot] => [試吓啦。]\n",
            "[Hows your job] => [你返工返成點呀？]\n",
            "[I like reading] => [我鍾意睇書。]\n",
            "[I made Tom cry] => [我整喊咗阿Tom。]\n",
            "[I study abroad] => [我喺外國讀書。]\n",
            "[Im a newcomer] => [我新嚟㗎。]\n",
            "[Im all for it] => [我舉腳贊成。]\n",
            "[Its too small] => [太細喇。]\n",
            "[May I help you] => [有咩幫到你呢？]\n",
            "[Say it clearly] => [講清楚啲。]\n",
            "[Shes not here] => [佢唔喺度。]\n",
            "[Thats obvious] => [鬼唔知咩。]\n",
            "[The man is old] => [個男人好老。]\n",
            "[They found out] => [佢哋發現咗喇。]\n",
            "[They let me go] => [佢哋俾我走。]\n",
            "[What a big dog] => [咁大隻狗嘅！]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL7Nd65d1Oe9",
        "outputId": "ef6c261c-32b8-4576-ee80-8e7d2b5da7c2"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-Cantonese.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 3255\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:3000], dataset[255:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-Cantonese-both.pkl')\n",
        "save_clean_data(train, 'english-Cantonese-train.pkl')\n",
        "save_clean_data(test, 'english-Cantonese-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Cantonese-both.pkl\n",
            "Saved: english-Cantonese-train.pkl\n",
            "Saved: english-Cantonese-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKMv68vH1Pec",
        "outputId": "03729d80-4de1-4a1a-c76b-92f4179ba92b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Cantonese-both.pkl')\n",
        "train = load_clean_sentences('english-Cantonese-train.pkl')\n",
        "test = load_clean_sentences('english-Cantonese-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Cantonese Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Cantonese  Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2671\n",
            "English Max Length: 32\n",
            "Cantonese Vocabulary Size: 3208\n",
            "Cantonese  Max Length: 4\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 256)            821248    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 32, 2671)          686447    \n",
            "=================================================================\n",
            "Total params: 2,558,319\n",
            "Trainable params: 2,558,319\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "47/47 - 41s - loss: 3.3608 - val_loss: 1.8445\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.84450, saving model to model.h5\n",
            "Epoch 2/100\n",
            "47/47 - 34s - loss: 1.8097 - val_loss: 1.8207\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.84450 to 1.82068, saving model to model.h5\n",
            "Epoch 3/100\n",
            "47/47 - 35s - loss: 1.7587 - val_loss: 1.7208\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.82068 to 1.72083, saving model to model.h5\n",
            "Epoch 4/100\n",
            "47/47 - 34s - loss: 1.7100 - val_loss: 1.8528\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.72083\n",
            "Epoch 5/100\n",
            "47/47 - 34s - loss: 1.6377 - val_loss: 1.5892\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.72083 to 1.58919, saving model to model.h5\n",
            "Epoch 6/100\n",
            "47/47 - 33s - loss: 1.5588 - val_loss: 1.5287\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.58919 to 1.52873, saving model to model.h5\n",
            "Epoch 7/100\n",
            "47/47 - 34s - loss: 1.5005 - val_loss: 1.4827\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.52873 to 1.48270, saving model to model.h5\n",
            "Epoch 8/100\n",
            "47/47 - 35s - loss: 1.4588 - val_loss: 1.4498\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.48270 to 1.44978, saving model to model.h5\n",
            "Epoch 9/100\n",
            "47/47 - 34s - loss: 1.4264 - val_loss: 1.4272\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.44978 to 1.42721, saving model to model.h5\n",
            "Epoch 10/100\n",
            "47/47 - 34s - loss: 1.3966 - val_loss: 1.3949\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.42721 to 1.39488, saving model to model.h5\n",
            "Epoch 11/100\n",
            "47/47 - 33s - loss: 1.3668 - val_loss: 1.3706\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.39488 to 1.37055, saving model to model.h5\n",
            "Epoch 12/100\n",
            "47/47 - 34s - loss: 1.3401 - val_loss: 1.3503\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.37055 to 1.35033, saving model to model.h5\n",
            "Epoch 13/100\n",
            "47/47 - 34s - loss: 1.3162 - val_loss: 1.3274\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.35033 to 1.32742, saving model to model.h5\n",
            "Epoch 14/100\n",
            "47/47 - 33s - loss: 1.2982 - val_loss: 1.3161\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.32742 to 1.31610, saving model to model.h5\n",
            "Epoch 15/100\n",
            "47/47 - 33s - loss: 1.2851 - val_loss: 1.3063\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.31610 to 1.30635, saving model to model.h5\n",
            "Epoch 16/100\n",
            "47/47 - 36s - loss: 1.2745 - val_loss: 1.2962\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.30635 to 1.29619, saving model to model.h5\n",
            "Epoch 17/100\n",
            "47/47 - 34s - loss: 1.2634 - val_loss: 1.2894\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.29619 to 1.28945, saving model to model.h5\n",
            "Epoch 18/100\n",
            "47/47 - 35s - loss: 1.2555 - val_loss: 1.2829\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.28945 to 1.28292, saving model to model.h5\n",
            "Epoch 19/100\n",
            "47/47 - 34s - loss: 1.2452 - val_loss: 1.2685\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.28292 to 1.26847, saving model to model.h5\n",
            "Epoch 20/100\n",
            "47/47 - 34s - loss: 1.2284 - val_loss: 1.2571\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.26847 to 1.25712, saving model to model.h5\n",
            "Epoch 21/100\n",
            "47/47 - 33s - loss: 1.2169 - val_loss: 1.2487\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.25712 to 1.24870, saving model to model.h5\n",
            "Epoch 22/100\n",
            "47/47 - 35s - loss: 1.2086 - val_loss: 1.2488\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.24870\n",
            "Epoch 23/100\n",
            "47/47 - 35s - loss: 1.2004 - val_loss: 1.2342\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.24870 to 1.23415, saving model to model.h5\n",
            "Epoch 24/100\n",
            "47/47 - 34s - loss: 1.1902 - val_loss: 1.2243\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.23415 to 1.22432, saving model to model.h5\n",
            "Epoch 25/100\n",
            "47/47 - 34s - loss: 1.1801 - val_loss: 1.2190\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.22432 to 1.21900, saving model to model.h5\n",
            "Epoch 26/100\n",
            "47/47 - 33s - loss: 1.1715 - val_loss: 1.2098\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.21900 to 1.20981, saving model to model.h5\n",
            "Epoch 27/100\n",
            "47/47 - 33s - loss: 1.1633 - val_loss: 1.2028\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.20981 to 1.20276, saving model to model.h5\n",
            "Epoch 28/100\n",
            "47/47 - 33s - loss: 1.1542 - val_loss: 1.1953\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.20276 to 1.19531, saving model to model.h5\n",
            "Epoch 29/100\n",
            "47/47 - 33s - loss: 1.1456 - val_loss: 1.1883\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.19531 to 1.18827, saving model to model.h5\n",
            "Epoch 30/100\n",
            "47/47 - 34s - loss: 1.1387 - val_loss: 1.1831\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.18827 to 1.18310, saving model to model.h5\n",
            "Epoch 31/100\n",
            "47/47 - 34s - loss: 1.1298 - val_loss: 1.1744\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.18310 to 1.17444, saving model to model.h5\n",
            "Epoch 32/100\n",
            "47/47 - 34s - loss: 1.1210 - val_loss: 1.1630\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.17444 to 1.16301, saving model to model.h5\n",
            "Epoch 33/100\n",
            "47/47 - 34s - loss: 1.1108 - val_loss: 1.1545\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.16301 to 1.15451, saving model to model.h5\n",
            "Epoch 34/100\n",
            "47/47 - 34s - loss: 1.0974 - val_loss: 1.1432\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.15451 to 1.14315, saving model to model.h5\n",
            "Epoch 35/100\n",
            "47/47 - 33s - loss: 1.0875 - val_loss: 1.1349\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.14315 to 1.13490, saving model to model.h5\n",
            "Epoch 36/100\n",
            "47/47 - 33s - loss: 1.0775 - val_loss: 1.1278\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.13490 to 1.12783, saving model to model.h5\n",
            "Epoch 37/100\n",
            "47/47 - 34s - loss: 1.0684 - val_loss: 1.1181\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.12783 to 1.11809, saving model to model.h5\n",
            "Epoch 38/100\n",
            "47/47 - 34s - loss: 1.0586 - val_loss: 1.1122\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.11809 to 1.11218, saving model to model.h5\n",
            "Epoch 39/100\n",
            "47/47 - 33s - loss: 1.0471 - val_loss: 1.0991\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.11218 to 1.09910, saving model to model.h5\n",
            "Epoch 40/100\n",
            "47/47 - 34s - loss: 1.0363 - val_loss: 1.0905\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.09910 to 1.09046, saving model to model.h5\n",
            "Epoch 41/100\n",
            "47/47 - 34s - loss: 1.0242 - val_loss: 1.0794\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.09046 to 1.07938, saving model to model.h5\n",
            "Epoch 42/100\n",
            "47/47 - 35s - loss: 1.0133 - val_loss: 1.0756\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.07938 to 1.07563, saving model to model.h5\n",
            "Epoch 43/100\n",
            "47/47 - 35s - loss: 1.0035 - val_loss: 1.0657\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.07563 to 1.06569, saving model to model.h5\n",
            "Epoch 44/100\n",
            "47/47 - 35s - loss: 0.9940 - val_loss: 1.0526\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.06569 to 1.05259, saving model to model.h5\n",
            "Epoch 45/100\n",
            "47/47 - 33s - loss: 0.9821 - val_loss: 1.0408\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.05259 to 1.04080, saving model to model.h5\n",
            "Epoch 46/100\n",
            "47/47 - 33s - loss: 0.9691 - val_loss: 1.0339\n",
            "\n",
            "Epoch 00046: val_loss improved from 1.04080 to 1.03390, saving model to model.h5\n",
            "Epoch 47/100\n",
            "47/47 - 33s - loss: 0.9584 - val_loss: 1.0253\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.03390 to 1.02534, saving model to model.h5\n",
            "Epoch 48/100\n",
            "47/47 - 33s - loss: 0.9478 - val_loss: 1.0129\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.02534 to 1.01293, saving model to model.h5\n",
            "Epoch 49/100\n",
            "47/47 - 34s - loss: 0.9365 - val_loss: 1.0022\n",
            "\n",
            "Epoch 00049: val_loss improved from 1.01293 to 1.00223, saving model to model.h5\n",
            "Epoch 50/100\n",
            "47/47 - 33s - loss: 0.9218 - val_loss: 0.9897\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.00223 to 0.98970, saving model to model.h5\n",
            "Epoch 51/100\n",
            "47/47 - 33s - loss: 0.9091 - val_loss: 0.9773\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.98970 to 0.97730, saving model to model.h5\n",
            "Epoch 52/100\n",
            "47/47 - 33s - loss: 0.8969 - val_loss: 0.9684\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.97730 to 0.96842, saving model to model.h5\n",
            "Epoch 53/100\n",
            "47/47 - 34s - loss: 0.8835 - val_loss: 0.9567\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.96842 to 0.95672, saving model to model.h5\n",
            "Epoch 54/100\n",
            "47/47 - 34s - loss: 0.8675 - val_loss: 0.9438\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.95672 to 0.94375, saving model to model.h5\n",
            "Epoch 55/100\n",
            "47/47 - 34s - loss: 0.8549 - val_loss: 0.9305\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.94375 to 0.93047, saving model to model.h5\n",
            "Epoch 56/100\n",
            "47/47 - 34s - loss: 0.8407 - val_loss: 0.9200\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.93047 to 0.92004, saving model to model.h5\n",
            "Epoch 57/100\n",
            "47/47 - 32s - loss: 0.8287 - val_loss: 0.9071\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.92004 to 0.90705, saving model to model.h5\n",
            "Epoch 58/100\n",
            "47/47 - 33s - loss: 0.8170 - val_loss: 0.9116\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.90705\n",
            "Epoch 59/100\n",
            "47/47 - 35s - loss: 0.8045 - val_loss: 0.8858\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.90705 to 0.88584, saving model to model.h5\n",
            "Epoch 60/100\n",
            "47/47 - 35s - loss: 0.7886 - val_loss: 0.8777\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.88584 to 0.87773, saving model to model.h5\n",
            "Epoch 61/100\n",
            "47/47 - 35s - loss: 0.7763 - val_loss: 0.8635\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.87773 to 0.86347, saving model to model.h5\n",
            "Epoch 62/100\n",
            "47/47 - 35s - loss: 0.7571 - val_loss: 0.8457\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.86347 to 0.84573, saving model to model.h5\n",
            "Epoch 63/100\n",
            "47/47 - 34s - loss: 0.7349 - val_loss: 0.8294\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.84573 to 0.82940, saving model to model.h5\n",
            "Epoch 64/100\n",
            "47/47 - 33s - loss: 0.7172 - val_loss: 0.8171\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.82940 to 0.81710, saving model to model.h5\n",
            "Epoch 65/100\n",
            "47/47 - 34s - loss: 0.7016 - val_loss: 0.8034\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.81710 to 0.80342, saving model to model.h5\n",
            "Epoch 66/100\n",
            "47/47 - 34s - loss: 0.6891 - val_loss: 0.7950\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.80342 to 0.79503, saving model to model.h5\n",
            "Epoch 67/100\n",
            "47/47 - 35s - loss: 0.6781 - val_loss: 0.7837\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.79503 to 0.78365, saving model to model.h5\n",
            "Epoch 68/100\n",
            "47/47 - 34s - loss: 0.6624 - val_loss: 0.7687\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.78365 to 0.76869, saving model to model.h5\n",
            "Epoch 69/100\n",
            "47/47 - 33s - loss: 0.6473 - val_loss: 0.7560\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.76869 to 0.75596, saving model to model.h5\n",
            "Epoch 70/100\n",
            "47/47 - 34s - loss: 0.6333 - val_loss: 0.7426\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.75596 to 0.74258, saving model to model.h5\n",
            "Epoch 71/100\n",
            "47/47 - 34s - loss: 0.6185 - val_loss: 0.7307\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.74258 to 0.73068, saving model to model.h5\n",
            "Epoch 72/100\n",
            "47/47 - 36s - loss: 0.6031 - val_loss: 0.7212\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.73068 to 0.72119, saving model to model.h5\n",
            "Epoch 73/100\n",
            "47/47 - 34s - loss: 0.5883 - val_loss: 0.7051\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.72119 to 0.70506, saving model to model.h5\n",
            "Epoch 74/100\n",
            "47/47 - 35s - loss: 0.5732 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.70506 to 0.69570, saving model to model.h5\n",
            "Epoch 75/100\n",
            "47/47 - 35s - loss: 0.5583 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.69570 to 0.68157, saving model to model.h5\n",
            "Epoch 76/100\n",
            "47/47 - 34s - loss: 0.5426 - val_loss: 0.6669\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.68157 to 0.66685, saving model to model.h5\n",
            "Epoch 77/100\n",
            "47/47 - 34s - loss: 0.5280 - val_loss: 0.6564\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.66685 to 0.65644, saving model to model.h5\n",
            "Epoch 78/100\n",
            "47/47 - 35s - loss: 0.5153 - val_loss: 0.6471\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.65644 to 0.64712, saving model to model.h5\n",
            "Epoch 79/100\n",
            "47/47 - 35s - loss: 0.5012 - val_loss: 0.6319\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.64712 to 0.63195, saving model to model.h5\n",
            "Epoch 80/100\n",
            "47/47 - 34s - loss: 0.4867 - val_loss: 0.6236\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.63195 to 0.62358, saving model to model.h5\n",
            "Epoch 81/100\n",
            "47/47 - 35s - loss: 0.4729 - val_loss: 0.6099\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.62358 to 0.60992, saving model to model.h5\n",
            "Epoch 82/100\n",
            "47/47 - 33s - loss: 0.4598 - val_loss: 0.5989\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.60992 to 0.59888, saving model to model.h5\n",
            "Epoch 83/100\n",
            "47/47 - 34s - loss: 0.4461 - val_loss: 0.5859\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.59888 to 0.58592, saving model to model.h5\n",
            "Epoch 84/100\n",
            "47/47 - 33s - loss: 0.4337 - val_loss: 0.5759\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.58592 to 0.57591, saving model to model.h5\n",
            "Epoch 85/100\n",
            "47/47 - 34s - loss: 0.4212 - val_loss: 0.5660\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.57591 to 0.56599, saving model to model.h5\n",
            "Epoch 86/100\n",
            "47/47 - 33s - loss: 0.4097 - val_loss: 0.5604\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.56599 to 0.56035, saving model to model.h5\n",
            "Epoch 87/100\n",
            "47/47 - 36s - loss: 0.4008 - val_loss: 0.5487\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.56035 to 0.54870, saving model to model.h5\n",
            "Epoch 88/100\n",
            "47/47 - 34s - loss: 0.3896 - val_loss: 0.5366\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.54870 to 0.53663, saving model to model.h5\n",
            "Epoch 89/100\n",
            "47/47 - 34s - loss: 0.3735 - val_loss: 0.5247\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.53663 to 0.52468, saving model to model.h5\n",
            "Epoch 90/100\n",
            "47/47 - 35s - loss: 0.3596 - val_loss: 0.5144\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.52468 to 0.51438, saving model to model.h5\n",
            "Epoch 91/100\n",
            "47/47 - 35s - loss: 0.3477 - val_loss: 0.5051\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.51438 to 0.50515, saving model to model.h5\n",
            "Epoch 92/100\n",
            "47/47 - 36s - loss: 0.3367 - val_loss: 0.4962\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.50515 to 0.49615, saving model to model.h5\n",
            "Epoch 93/100\n",
            "47/47 - 35s - loss: 0.3255 - val_loss: 0.4873\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.49615 to 0.48732, saving model to model.h5\n",
            "Epoch 94/100\n",
            "47/47 - 36s - loss: 0.3144 - val_loss: 0.4817\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.48732 to 0.48166, saving model to model.h5\n",
            "Epoch 95/100\n",
            "47/47 - 35s - loss: 0.3059 - val_loss: 0.4715\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.48166 to 0.47149, saving model to model.h5\n",
            "Epoch 96/100\n",
            "47/47 - 36s - loss: 0.3002 - val_loss: 0.4703\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.47149 to 0.47030, saving model to model.h5\n",
            "Epoch 97/100\n",
            "47/47 - 34s - loss: 0.2934 - val_loss: 0.4626\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.47030 to 0.46257, saving model to model.h5\n",
            "Epoch 98/100\n",
            "47/47 - 34s - loss: 0.2856 - val_loss: 0.4563\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.46257 to 0.45629, saving model to model.h5\n",
            "Epoch 99/100\n",
            "47/47 - 35s - loss: 0.2745 - val_loss: 0.4446\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.45629 to 0.44456, saving model to model.h5\n",
            "Epoch 100/100\n",
            "47/47 - 35s - loss: 0.2606 - val_loss: 0.4330\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.44456 to 0.43303, saving model to model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9260629f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUZ1GLfCa-nc",
        "outputId": "bd24e257-34ed-40b8-f838-fb2e599e4d7d"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Cantonese-both.pkl')\n",
        "train = load_clean_sentences('english-Cantonese-train.pkl')\n",
        "test = load_clean_sentences('english-Cantonese-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Cantonese Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Cantonese  Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=400, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2671\n",
            "English Max Length: 32\n",
            "Cantonese Vocabulary Size: 3208\n",
            "Cantonese  Max Length: 4\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 256)            821248    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 32, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 32, 2671)          686447    \n",
            "=================================================================\n",
            "Total params: 2,558,319\n",
            "Trainable params: 2,558,319\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/400\n",
            "47/47 - 39s - loss: 3.3432 - val_loss: 1.8379\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.83788, saving model to model.h5\n",
            "Epoch 2/400\n",
            "47/47 - 33s - loss: 1.8116 - val_loss: 1.8642\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.83788\n",
            "Epoch 3/400\n",
            "47/47 - 33s - loss: 1.7804 - val_loss: 2.0477\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.83788\n",
            "Epoch 4/400\n",
            "47/47 - 33s - loss: 1.7093 - val_loss: 1.6497\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.83788 to 1.64966, saving model to model.h5\n",
            "Epoch 5/400\n",
            "47/47 - 34s - loss: 1.6230 - val_loss: 1.5879\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.64966 to 1.58790, saving model to model.h5\n",
            "Epoch 6/400\n",
            "47/47 - 34s - loss: 1.5537 - val_loss: 1.5226\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.58790 to 1.52259, saving model to model.h5\n",
            "Epoch 7/400\n",
            "47/47 - 33s - loss: 1.5003 - val_loss: 1.4813\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.52259 to 1.48129, saving model to model.h5\n",
            "Epoch 8/400\n",
            "47/47 - 32s - loss: 1.4594 - val_loss: 1.4504\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.48129 to 1.45044, saving model to model.h5\n",
            "Epoch 9/400\n",
            "47/47 - 33s - loss: 1.4242 - val_loss: 1.4109\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.45044 to 1.41089, saving model to model.h5\n",
            "Epoch 10/400\n",
            "47/47 - 33s - loss: 1.3869 - val_loss: 1.3807\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.41089 to 1.38065, saving model to model.h5\n",
            "Epoch 11/400\n",
            "47/47 - 33s - loss: 1.3554 - val_loss: 1.3573\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.38065 to 1.35725, saving model to model.h5\n",
            "Epoch 12/400\n",
            "47/47 - 33s - loss: 1.3350 - val_loss: 1.3403\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.35725 to 1.34026, saving model to model.h5\n",
            "Epoch 13/400\n",
            "47/47 - 33s - loss: 1.3162 - val_loss: 1.3242\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.34026 to 1.32419, saving model to model.h5\n",
            "Epoch 14/400\n",
            "47/47 - 33s - loss: 1.2963 - val_loss: 1.3128\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.32419 to 1.31281, saving model to model.h5\n",
            "Epoch 15/400\n",
            "47/47 - 33s - loss: 1.2831 - val_loss: 1.3035\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.31281 to 1.30350, saving model to model.h5\n",
            "Epoch 16/400\n",
            "47/47 - 33s - loss: 1.2738 - val_loss: 1.2950\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.30350 to 1.29504, saving model to model.h5\n",
            "Epoch 17/400\n",
            "47/47 - 33s - loss: 1.2654 - val_loss: 1.2987\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.29504\n",
            "Epoch 18/400\n",
            "47/47 - 32s - loss: 1.2598 - val_loss: 1.2785\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.29504 to 1.27853, saving model to model.h5\n",
            "Epoch 19/400\n",
            "47/47 - 32s - loss: 1.2448 - val_loss: 1.2689\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.27853 to 1.26895, saving model to model.h5\n",
            "Epoch 20/400\n",
            "47/47 - 33s - loss: 1.2324 - val_loss: 1.2568\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.26895 to 1.25677, saving model to model.h5\n",
            "Epoch 21/400\n",
            "47/47 - 33s - loss: 1.2222 - val_loss: 1.2522\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.25677 to 1.25219, saving model to model.h5\n",
            "Epoch 22/400\n",
            "47/47 - 33s - loss: 1.2139 - val_loss: 1.2428\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.25219 to 1.24280, saving model to model.h5\n",
            "Epoch 23/400\n",
            "47/47 - 32s - loss: 1.2014 - val_loss: 1.2316\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.24280 to 1.23164, saving model to model.h5\n",
            "Epoch 24/400\n",
            "47/47 - 33s - loss: 1.1933 - val_loss: 1.2317\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.23164\n",
            "Epoch 25/400\n",
            "47/47 - 33s - loss: 1.1877 - val_loss: 1.2195\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.23164 to 1.21946, saving model to model.h5\n",
            "Epoch 26/400\n",
            "47/47 - 34s - loss: 1.1775 - val_loss: 1.2149\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.21946 to 1.21488, saving model to model.h5\n",
            "Epoch 27/400\n",
            "47/47 - 34s - loss: 1.1685 - val_loss: 1.2046\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.21488 to 1.20465, saving model to model.h5\n",
            "Epoch 28/400\n",
            "47/47 - 34s - loss: 1.1619 - val_loss: 1.2047\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.20465\n",
            "Epoch 29/400\n",
            "47/47 - 33s - loss: 1.1573 - val_loss: 1.1970\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.20465 to 1.19695, saving model to model.h5\n",
            "Epoch 30/400\n",
            "47/47 - 33s - loss: 1.1500 - val_loss: 1.1887\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.19695 to 1.18869, saving model to model.h5\n",
            "Epoch 31/400\n",
            "47/47 - 34s - loss: 1.1403 - val_loss: 1.1774\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.18869 to 1.17745, saving model to model.h5\n",
            "Epoch 32/400\n",
            "47/47 - 34s - loss: 1.1304 - val_loss: 1.1689\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.17745 to 1.16891, saving model to model.h5\n",
            "Epoch 33/400\n",
            "47/47 - 34s - loss: 1.1194 - val_loss: 1.1615\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.16891 to 1.16151, saving model to model.h5\n",
            "Epoch 34/400\n",
            "47/47 - 34s - loss: 1.1116 - val_loss: 1.1524\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.16151 to 1.15244, saving model to model.h5\n",
            "Epoch 35/400\n",
            "47/47 - 34s - loss: 1.1036 - val_loss: 1.1451\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.15244 to 1.14509, saving model to model.h5\n",
            "Epoch 36/400\n",
            "47/47 - 34s - loss: 1.0964 - val_loss: 1.1418\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.14509 to 1.14178, saving model to model.h5\n",
            "Epoch 37/400\n",
            "47/47 - 35s - loss: 1.0865 - val_loss: 1.1279\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.14178 to 1.12790, saving model to model.h5\n",
            "Epoch 38/400\n",
            "47/47 - 34s - loss: 1.0732 - val_loss: 1.1158\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.12790 to 1.11585, saving model to model.h5\n",
            "Epoch 39/400\n",
            "47/47 - 35s - loss: 1.0606 - val_loss: 1.1042\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.11585 to 1.10424, saving model to model.h5\n",
            "Epoch 40/400\n",
            "47/47 - 36s - loss: 1.0488 - val_loss: 1.0970\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.10424 to 1.09702, saving model to model.h5\n",
            "Epoch 41/400\n",
            "47/47 - 34s - loss: 1.0397 - val_loss: 1.0907\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.09702 to 1.09069, saving model to model.h5\n",
            "Epoch 42/400\n",
            "47/47 - 34s - loss: 1.0314 - val_loss: 1.0791\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.09069 to 1.07912, saving model to model.h5\n",
            "Epoch 43/400\n",
            "47/47 - 34s - loss: 1.0225 - val_loss: 1.0724\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.07912 to 1.07242, saving model to model.h5\n",
            "Epoch 44/400\n",
            "47/47 - 34s - loss: 1.0113 - val_loss: 1.0591\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.07242 to 1.05912, saving model to model.h5\n",
            "Epoch 45/400\n",
            "47/47 - 34s - loss: 0.9957 - val_loss: 1.0483\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.05912 to 1.04826, saving model to model.h5\n",
            "Epoch 46/400\n",
            "47/47 - 35s - loss: 0.9825 - val_loss: 1.0372\n",
            "\n",
            "Epoch 00046: val_loss improved from 1.04826 to 1.03717, saving model to model.h5\n",
            "Epoch 47/400\n",
            "47/47 - 34s - loss: 0.9723 - val_loss: 1.0310\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.03717 to 1.03100, saving model to model.h5\n",
            "Epoch 48/400\n",
            "47/47 - 35s - loss: 0.9630 - val_loss: 1.0198\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.03100 to 1.01985, saving model to model.h5\n",
            "Epoch 49/400\n",
            "47/47 - 35s - loss: 0.9503 - val_loss: 1.0112\n",
            "\n",
            "Epoch 00049: val_loss improved from 1.01985 to 1.01124, saving model to model.h5\n",
            "Epoch 50/400\n",
            "47/47 - 36s - loss: 0.9386 - val_loss: 0.9977\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.01124 to 0.99775, saving model to model.h5\n",
            "Epoch 51/400\n",
            "47/47 - 35s - loss: 0.9289 - val_loss: 0.9899\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.99775 to 0.98989, saving model to model.h5\n",
            "Epoch 52/400\n",
            "47/47 - 35s - loss: 0.9164 - val_loss: 0.9814\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.98989 to 0.98139, saving model to model.h5\n",
            "Epoch 53/400\n",
            "47/47 - 34s - loss: 0.9046 - val_loss: 0.9677\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.98139 to 0.96774, saving model to model.h5\n",
            "Epoch 54/400\n",
            "47/47 - 35s - loss: 0.8916 - val_loss: 0.9605\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.96774 to 0.96054, saving model to model.h5\n",
            "Epoch 55/400\n",
            "47/47 - 35s - loss: 0.8789 - val_loss: 0.9451\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.96054 to 0.94511, saving model to model.h5\n",
            "Epoch 56/400\n",
            "47/47 - 35s - loss: 0.8670 - val_loss: 0.9394\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.94511 to 0.93944, saving model to model.h5\n",
            "Epoch 57/400\n",
            "47/47 - 35s - loss: 0.8507 - val_loss: 0.9215\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.93944 to 0.92149, saving model to model.h5\n",
            "Epoch 58/400\n",
            "47/47 - 36s - loss: 0.8372 - val_loss: 0.9134\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.92149 to 0.91338, saving model to model.h5\n",
            "Epoch 59/400\n",
            "47/47 - 37s - loss: 0.8261 - val_loss: 0.9022\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.91338 to 0.90221, saving model to model.h5\n",
            "Epoch 60/400\n",
            "47/47 - 35s - loss: 0.8182 - val_loss: 0.8880\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.90221 to 0.88802, saving model to model.h5\n",
            "Epoch 61/400\n",
            "47/47 - 34s - loss: 0.7986 - val_loss: 0.8739\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.88802 to 0.87386, saving model to model.h5\n",
            "Epoch 62/400\n",
            "47/47 - 35s - loss: 0.7800 - val_loss: 0.8568\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.87386 to 0.85679, saving model to model.h5\n",
            "Epoch 63/400\n",
            "47/47 - 35s - loss: 0.7597 - val_loss: 0.8432\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.85679 to 0.84321, saving model to model.h5\n",
            "Epoch 64/400\n",
            "47/47 - 35s - loss: 0.7420 - val_loss: 0.8300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.84321 to 0.83003, saving model to model.h5\n",
            "Epoch 65/400\n",
            "47/47 - 36s - loss: 0.7286 - val_loss: 0.8176\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.83003 to 0.81757, saving model to model.h5\n",
            "Epoch 66/400\n",
            "47/47 - 36s - loss: 0.7161 - val_loss: 0.8113\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.81757 to 0.81126, saving model to model.h5\n",
            "Epoch 67/400\n",
            "47/47 - 35s - loss: 0.7021 - val_loss: 0.7917\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.81126 to 0.79172, saving model to model.h5\n",
            "Epoch 68/400\n",
            "47/47 - 35s - loss: 0.6850 - val_loss: 0.7780\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.79172 to 0.77797, saving model to model.h5\n",
            "Epoch 69/400\n",
            "47/47 - 36s - loss: 0.6682 - val_loss: 0.7648\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.77797 to 0.76481, saving model to model.h5\n",
            "Epoch 70/400\n",
            "47/47 - 36s - loss: 0.6529 - val_loss: 0.7549\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.76481 to 0.75492, saving model to model.h5\n",
            "Epoch 71/400\n",
            "47/47 - 35s - loss: 0.6376 - val_loss: 0.7400\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.75492 to 0.74001, saving model to model.h5\n",
            "Epoch 72/400\n",
            "47/47 - 35s - loss: 0.6229 - val_loss: 0.7253\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.74001 to 0.72526, saving model to model.h5\n",
            "Epoch 73/400\n",
            "47/47 - 36s - loss: 0.6065 - val_loss: 0.7119\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.72526 to 0.71187, saving model to model.h5\n",
            "Epoch 74/400\n",
            "47/47 - 37s - loss: 0.5913 - val_loss: 0.7029\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.71187 to 0.70285, saving model to model.h5\n",
            "Epoch 75/400\n",
            "47/47 - 36s - loss: 0.5786 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.70285 to 0.69300, saving model to model.h5\n",
            "Epoch 76/400\n",
            "47/47 - 37s - loss: 0.5662 - val_loss: 0.6792\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.69300 to 0.67922, saving model to model.h5\n",
            "Epoch 77/400\n",
            "47/47 - 36s - loss: 0.5536 - val_loss: 0.6694\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.67922 to 0.66944, saving model to model.h5\n",
            "Epoch 78/400\n",
            "47/47 - 36s - loss: 0.5395 - val_loss: 0.6573\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.66944 to 0.65734, saving model to model.h5\n",
            "Epoch 79/400\n",
            "47/47 - 37s - loss: 0.5228 - val_loss: 0.6429\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.65734 to 0.64285, saving model to model.h5\n",
            "Epoch 80/400\n",
            "47/47 - 36s - loss: 0.5080 - val_loss: 0.6325\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.64285 to 0.63250, saving model to model.h5\n",
            "Epoch 81/400\n",
            "47/47 - 37s - loss: 0.4925 - val_loss: 0.6201\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.63250 to 0.62011, saving model to model.h5\n",
            "Epoch 82/400\n",
            "47/47 - 37s - loss: 0.4802 - val_loss: 0.6110\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.62011 to 0.61103, saving model to model.h5\n",
            "Epoch 83/400\n",
            "47/47 - 36s - loss: 0.4697 - val_loss: 0.6005\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.61103 to 0.60050, saving model to model.h5\n",
            "Epoch 84/400\n",
            "47/47 - 35s - loss: 0.4555 - val_loss: 0.5873\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.60050 to 0.58726, saving model to model.h5\n",
            "Epoch 85/400\n",
            "47/47 - 35s - loss: 0.4404 - val_loss: 0.5744\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.58726 to 0.57436, saving model to model.h5\n",
            "Epoch 86/400\n",
            "47/47 - 35s - loss: 0.4271 - val_loss: 0.5690\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.57436 to 0.56904, saving model to model.h5\n",
            "Epoch 87/400\n",
            "47/47 - 34s - loss: 0.4145 - val_loss: 0.5567\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.56904 to 0.55666, saving model to model.h5\n",
            "Epoch 88/400\n",
            "47/47 - 35s - loss: 0.4028 - val_loss: 0.5453\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.55666 to 0.54533, saving model to model.h5\n",
            "Epoch 89/400\n",
            "47/47 - 34s - loss: 0.3913 - val_loss: 0.5382\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.54533 to 0.53816, saving model to model.h5\n",
            "Epoch 90/400\n",
            "47/47 - 35s - loss: 0.3796 - val_loss: 0.5258\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.53816 to 0.52580, saving model to model.h5\n",
            "Epoch 91/400\n",
            "47/47 - 35s - loss: 0.3697 - val_loss: 0.5199\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.52580 to 0.51993, saving model to model.h5\n",
            "Epoch 92/400\n",
            "47/47 - 35s - loss: 0.3621 - val_loss: 0.5132\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.51993 to 0.51322, saving model to model.h5\n",
            "Epoch 93/400\n",
            "47/47 - 35s - loss: 0.3511 - val_loss: 0.5046\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.51322 to 0.50456, saving model to model.h5\n",
            "Epoch 94/400\n",
            "47/47 - 35s - loss: 0.3390 - val_loss: 0.4910\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.50456 to 0.49102, saving model to model.h5\n",
            "Epoch 95/400\n",
            "47/47 - 34s - loss: 0.3255 - val_loss: 0.4798\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.49102 to 0.47980, saving model to model.h5\n",
            "Epoch 96/400\n",
            "47/47 - 34s - loss: 0.3128 - val_loss: 0.4755\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.47980 to 0.47547, saving model to model.h5\n",
            "Epoch 97/400\n",
            "47/47 - 34s - loss: 0.3036 - val_loss: 0.4650\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.47547 to 0.46505, saving model to model.h5\n",
            "Epoch 98/400\n",
            "47/47 - 34s - loss: 0.2946 - val_loss: 0.4581\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.46505 to 0.45809, saving model to model.h5\n",
            "Epoch 99/400\n",
            "47/47 - 34s - loss: 0.2855 - val_loss: 0.4520\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.45809 to 0.45199, saving model to model.h5\n",
            "Epoch 100/400\n",
            "47/47 - 34s - loss: 0.2777 - val_loss: 0.4456\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.45199 to 0.44562, saving model to model.h5\n",
            "Epoch 101/400\n",
            "47/47 - 35s - loss: 0.2702 - val_loss: 0.4388\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.44562 to 0.43880, saving model to model.h5\n",
            "Epoch 102/400\n",
            "47/47 - 35s - loss: 0.2601 - val_loss: 0.4307\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.43880 to 0.43067, saving model to model.h5\n",
            "Epoch 103/400\n",
            "47/47 - 36s - loss: 0.2473 - val_loss: 0.4185\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.43067 to 0.41855, saving model to model.h5\n",
            "Epoch 104/400\n",
            "47/47 - 37s - loss: 0.2362 - val_loss: 0.4139\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.41855 to 0.41391, saving model to model.h5\n",
            "Epoch 105/400\n",
            "47/47 - 34s - loss: 0.2273 - val_loss: 0.4061\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.41391 to 0.40606, saving model to model.h5\n",
            "Epoch 106/400\n",
            "47/47 - 35s - loss: 0.2212 - val_loss: 0.4008\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.40606 to 0.40077, saving model to model.h5\n",
            "Epoch 107/400\n",
            "47/47 - 35s - loss: 0.2141 - val_loss: 0.3931\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.40077 to 0.39314, saving model to model.h5\n",
            "Epoch 108/400\n",
            "47/47 - 34s - loss: 0.2076 - val_loss: 0.3922\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.39314 to 0.39220, saving model to model.h5\n",
            "Epoch 109/400\n",
            "47/47 - 36s - loss: 0.2021 - val_loss: 0.3832\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.39220 to 0.38320, saving model to model.h5\n",
            "Epoch 110/400\n",
            "47/47 - 35s - loss: 0.1945 - val_loss: 0.3800\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.38320 to 0.38002, saving model to model.h5\n",
            "Epoch 111/400\n",
            "47/47 - 35s - loss: 0.1888 - val_loss: 0.3753\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.38002 to 0.37527, saving model to model.h5\n",
            "Epoch 112/400\n",
            "47/47 - 35s - loss: 0.1827 - val_loss: 0.3705\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.37527 to 0.37052, saving model to model.h5\n",
            "Epoch 113/400\n",
            "47/47 - 34s - loss: 0.1758 - val_loss: 0.3642\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.37052 to 0.36419, saving model to model.h5\n",
            "Epoch 114/400\n",
            "47/47 - 35s - loss: 0.1696 - val_loss: 0.3600\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.36419 to 0.36005, saving model to model.h5\n",
            "Epoch 115/400\n",
            "47/47 - 35s - loss: 0.1650 - val_loss: 0.3581\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.36005 to 0.35806, saving model to model.h5\n",
            "Epoch 116/400\n",
            "47/47 - 35s - loss: 0.1600 - val_loss: 0.3524\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.35806 to 0.35240, saving model to model.h5\n",
            "Epoch 117/400\n",
            "47/47 - 35s - loss: 0.1538 - val_loss: 0.3492\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.35240 to 0.34922, saving model to model.h5\n",
            "Epoch 118/400\n",
            "47/47 - 34s - loss: 0.1478 - val_loss: 0.3443\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.34922 to 0.34427, saving model to model.h5\n",
            "Epoch 119/400\n",
            "47/47 - 34s - loss: 0.1416 - val_loss: 0.3364\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.34427 to 0.33644, saving model to model.h5\n",
            "Epoch 120/400\n",
            "47/47 - 34s - loss: 0.1356 - val_loss: 0.3350\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.33644 to 0.33500, saving model to model.h5\n",
            "Epoch 121/400\n",
            "47/47 - 34s - loss: 0.1321 - val_loss: 0.3312\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.33500 to 0.33124, saving model to model.h5\n",
            "Epoch 122/400\n",
            "47/47 - 34s - loss: 0.1275 - val_loss: 0.3271\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.33124 to 0.32712, saving model to model.h5\n",
            "Epoch 123/400\n",
            "47/47 - 34s - loss: 0.1212 - val_loss: 0.3273\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.32712\n",
            "Epoch 124/400\n",
            "47/47 - 34s - loss: 0.1180 - val_loss: 0.3242\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.32712 to 0.32425, saving model to model.h5\n",
            "Epoch 125/400\n",
            "47/47 - 34s - loss: 0.1161 - val_loss: 0.3173\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.32425 to 0.31730, saving model to model.h5\n",
            "Epoch 126/400\n",
            "47/47 - 34s - loss: 0.1121 - val_loss: 0.3145\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.31730 to 0.31452, saving model to model.h5\n",
            "Epoch 127/400\n",
            "47/47 - 34s - loss: 0.1063 - val_loss: 0.3131\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.31452 to 0.31313, saving model to model.h5\n",
            "Epoch 128/400\n",
            "47/47 - 34s - loss: 0.1031 - val_loss: 0.3108\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.31313 to 0.31077, saving model to model.h5\n",
            "Epoch 129/400\n",
            "47/47 - 34s - loss: 0.1015 - val_loss: 0.3098\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.31077 to 0.30978, saving model to model.h5\n",
            "Epoch 130/400\n",
            "47/47 - 34s - loss: 0.1003 - val_loss: 0.3070\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.30978 to 0.30702, saving model to model.h5\n",
            "Epoch 131/400\n",
            "47/47 - 34s - loss: 0.0969 - val_loss: 0.3046\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.30702 to 0.30463, saving model to model.h5\n",
            "Epoch 132/400\n",
            "47/47 - 34s - loss: 0.0933 - val_loss: 0.3033\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.30463 to 0.30331, saving model to model.h5\n",
            "Epoch 133/400\n",
            "47/47 - 34s - loss: 0.0889 - val_loss: 0.2999\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.30331 to 0.29988, saving model to model.h5\n",
            "Epoch 134/400\n",
            "47/47 - 34s - loss: 0.0864 - val_loss: 0.2972\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.29988 to 0.29724, saving model to model.h5\n",
            "Epoch 135/400\n",
            "47/47 - 34s - loss: 0.0814 - val_loss: 0.2928\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.29724 to 0.29278, saving model to model.h5\n",
            "Epoch 136/400\n",
            "47/47 - 34s - loss: 0.0769 - val_loss: 0.2916\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.29278 to 0.29162, saving model to model.h5\n",
            "Epoch 137/400\n",
            "47/47 - 34s - loss: 0.0736 - val_loss: 0.2952\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.29162\n",
            "Epoch 138/400\n",
            "47/47 - 34s - loss: 0.0723 - val_loss: 0.2884\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.29162 to 0.28838, saving model to model.h5\n",
            "Epoch 139/400\n",
            "47/47 - 34s - loss: 0.0705 - val_loss: 0.2870\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.28838 to 0.28704, saving model to model.h5\n",
            "Epoch 140/400\n",
            "47/47 - 34s - loss: 0.0684 - val_loss: 0.2943\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.28704\n",
            "Epoch 141/400\n",
            "47/47 - 34s - loss: 0.0672 - val_loss: 0.2864\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.28704 to 0.28639, saving model to model.h5\n",
            "Epoch 142/400\n",
            "47/47 - 34s - loss: 0.0644 - val_loss: 0.2858\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.28639 to 0.28576, saving model to model.h5\n",
            "Epoch 143/400\n",
            "47/47 - 34s - loss: 0.0623 - val_loss: 0.2845\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.28576 to 0.28454, saving model to model.h5\n",
            "Epoch 144/400\n",
            "47/47 - 35s - loss: 0.0592 - val_loss: 0.2809\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.28454 to 0.28086, saving model to model.h5\n",
            "Epoch 145/400\n",
            "47/47 - 35s - loss: 0.0590 - val_loss: 0.2869\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.28086\n",
            "Epoch 146/400\n",
            "47/47 - 35s - loss: 0.0630 - val_loss: 0.2842\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.28086\n",
            "Epoch 147/400\n",
            "47/47 - 34s - loss: 0.0651 - val_loss: 0.2843\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.28086\n",
            "Epoch 148/400\n",
            "47/47 - 35s - loss: 0.0614 - val_loss: 0.2820\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.28086\n",
            "Epoch 149/400\n",
            "47/47 - 35s - loss: 0.0584 - val_loss: 0.2820\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.28086\n",
            "Epoch 150/400\n",
            "47/47 - 34s - loss: 0.0553 - val_loss: 0.2778\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.28086 to 0.27775, saving model to model.h5\n",
            "Epoch 151/400\n",
            "47/47 - 35s - loss: 0.0531 - val_loss: 0.2764\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.27775 to 0.27637, saving model to model.h5\n",
            "Epoch 152/400\n",
            "47/47 - 34s - loss: 0.0506 - val_loss: 0.2743\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.27637 to 0.27429, saving model to model.h5\n",
            "Epoch 153/400\n",
            "47/47 - 33s - loss: 0.0484 - val_loss: 0.2712\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.27429 to 0.27116, saving model to model.h5\n",
            "Epoch 154/400\n",
            "47/47 - 33s - loss: 0.0450 - val_loss: 0.2721\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.27116\n",
            "Epoch 155/400\n",
            "47/47 - 33s - loss: 0.0438 - val_loss: 0.2708\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.27116 to 0.27083, saving model to model.h5\n",
            "Epoch 156/400\n",
            "47/47 - 33s - loss: 0.0442 - val_loss: 0.2789\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.27083\n",
            "Epoch 157/400\n",
            "47/47 - 34s - loss: 0.0565 - val_loss: 0.3357\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.27083\n",
            "Epoch 158/400\n",
            "47/47 - 35s - loss: 0.0737 - val_loss: 0.2853\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.27083\n",
            "Epoch 159/400\n",
            "47/47 - 35s - loss: 0.0582 - val_loss: 0.2829\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.27083\n",
            "Epoch 160/400\n",
            "47/47 - 35s - loss: 0.0543 - val_loss: 0.2820\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.27083\n",
            "Epoch 161/400\n",
            "47/47 - 34s - loss: 0.0528 - val_loss: 0.2881\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.27083\n",
            "Epoch 162/400\n",
            "47/47 - 35s - loss: 0.0528 - val_loss: 0.2742\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.27083\n",
            "Epoch 163/400\n",
            "47/47 - 35s - loss: 0.0475 - val_loss: 0.2723\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.27083\n",
            "Epoch 164/400\n",
            "47/47 - 35s - loss: 0.0418 - val_loss: 0.2726\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.27083\n",
            "Epoch 165/400\n",
            "47/47 - 34s - loss: 0.0388 - val_loss: 0.2661\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.27083 to 0.26614, saving model to model.h5\n",
            "Epoch 166/400\n",
            "47/47 - 34s - loss: 0.0367 - val_loss: 0.2648\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.26614 to 0.26482, saving model to model.h5\n",
            "Epoch 167/400\n",
            "47/47 - 33s - loss: 0.0339 - val_loss: 0.2653\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.26482\n",
            "Epoch 168/400\n",
            "47/47 - 33s - loss: 0.0311 - val_loss: 0.2619\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.26482 to 0.26193, saving model to model.h5\n",
            "Epoch 169/400\n",
            "47/47 - 33s - loss: 0.0289 - val_loss: 0.2602\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.26193 to 0.26019, saving model to model.h5\n",
            "Epoch 170/400\n",
            "47/47 - 34s - loss: 0.0274 - val_loss: 0.2613\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.26019\n",
            "Epoch 171/400\n",
            "47/47 - 33s - loss: 0.0263 - val_loss: 0.2613\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.26019\n",
            "Epoch 172/400\n",
            "47/47 - 33s - loss: 0.0271 - val_loss: 0.2611\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.26019\n",
            "Epoch 173/400\n",
            "47/47 - 34s - loss: 0.0284 - val_loss: 0.2618\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.26019\n",
            "Epoch 174/400\n",
            "47/47 - 33s - loss: 0.0304 - val_loss: 0.2657\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.26019\n",
            "Epoch 175/400\n",
            "47/47 - 33s - loss: 0.0307 - val_loss: 0.2628\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.26019\n",
            "Epoch 176/400\n",
            "47/47 - 33s - loss: 0.0308 - val_loss: 0.2672\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.26019\n",
            "Epoch 177/400\n",
            "47/47 - 33s - loss: 0.0337 - val_loss: 0.2657\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.26019\n",
            "Epoch 178/400\n",
            "47/47 - 33s - loss: 0.0327 - val_loss: 0.2614\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.26019\n",
            "Epoch 179/400\n",
            "47/47 - 34s - loss: 0.0282 - val_loss: 0.2656\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.26019\n",
            "Epoch 180/400\n",
            "47/47 - 34s - loss: 0.0273 - val_loss: 0.2628\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.26019\n",
            "Epoch 181/400\n",
            "47/47 - 34s - loss: 0.0264 - val_loss: 0.2627\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.26019\n",
            "Epoch 182/400\n",
            "47/47 - 34s - loss: 0.0242 - val_loss: 0.2601\n",
            "\n",
            "Epoch 00182: val_loss improved from 0.26019 to 0.26008, saving model to model.h5\n",
            "Epoch 183/400\n",
            "47/47 - 34s - loss: 0.0231 - val_loss: 0.2606\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.26008\n",
            "Epoch 184/400\n",
            "47/47 - 34s - loss: 0.0233 - val_loss: 0.2607\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.26008\n",
            "Epoch 185/400\n",
            "47/47 - 37s - loss: 0.0254 - val_loss: 0.2617\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.26008\n",
            "Epoch 186/400\n",
            "47/47 - 38s - loss: 0.0281 - val_loss: 0.2678\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.26008\n",
            "Epoch 187/400\n",
            "47/47 - 38s - loss: 0.0274 - val_loss: 0.2650\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.26008\n",
            "Epoch 188/400\n",
            "47/47 - 38s - loss: 0.0290 - val_loss: 0.2682\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.26008\n",
            "Epoch 189/400\n",
            "47/47 - 37s - loss: 0.0300 - val_loss: 0.2666\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.26008\n",
            "Epoch 190/400\n",
            "47/47 - 38s - loss: 0.0312 - val_loss: 0.2685\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.26008\n",
            "Epoch 191/400\n",
            "47/47 - 38s - loss: 0.0340 - val_loss: 0.2740\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.26008\n",
            "Epoch 192/400\n",
            "47/47 - 38s - loss: 0.0409 - val_loss: 0.2773\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.26008\n",
            "Epoch 193/400\n",
            "47/47 - 38s - loss: 0.0413 - val_loss: 0.2728\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.26008\n",
            "Epoch 194/400\n",
            "47/47 - 38s - loss: 0.0403 - val_loss: 0.2702\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.26008\n",
            "Epoch 195/400\n",
            "47/47 - 38s - loss: 0.0376 - val_loss: 0.2702\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.26008\n",
            "Epoch 196/400\n",
            "47/47 - 38s - loss: 0.0335 - val_loss: 0.2696\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.26008\n",
            "Epoch 197/400\n",
            "47/47 - 38s - loss: 0.0319 - val_loss: 0.2680\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.26008\n",
            "Epoch 198/400\n",
            "47/47 - 38s - loss: 0.0289 - val_loss: 0.2645\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.26008\n",
            "Epoch 199/400\n",
            "47/47 - 38s - loss: 0.0241 - val_loss: 0.2645\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.26008\n",
            "Epoch 200/400\n",
            "47/47 - 37s - loss: 0.0208 - val_loss: 0.2610\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.26008\n",
            "Epoch 201/400\n",
            "47/47 - 37s - loss: 0.0189 - val_loss: 0.2576\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.26008 to 0.25760, saving model to model.h5\n",
            "Epoch 202/400\n",
            "47/47 - 38s - loss: 0.0167 - val_loss: 0.2608\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.25760\n",
            "Epoch 203/400\n",
            "47/47 - 38s - loss: 0.0158 - val_loss: 0.2570\n",
            "\n",
            "Epoch 00203: val_loss improved from 0.25760 to 0.25703, saving model to model.h5\n",
            "Epoch 204/400\n",
            "47/47 - 38s - loss: 0.0153 - val_loss: 0.2592\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.25703\n",
            "Epoch 205/400\n",
            "47/47 - 37s - loss: 0.0147 - val_loss: 0.2579\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.25703\n",
            "Epoch 206/400\n",
            "47/47 - 38s - loss: 0.0146 - val_loss: 0.2583\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.25703\n",
            "Epoch 207/400\n",
            "47/47 - 37s - loss: 0.0156 - val_loss: 0.2588\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.25703\n",
            "Epoch 208/400\n",
            "47/47 - 37s - loss: 0.0163 - val_loss: 0.2597\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.25703\n",
            "Epoch 209/400\n",
            "47/47 - 37s - loss: 0.0167 - val_loss: 0.2617\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.25703\n",
            "Epoch 210/400\n",
            "47/47 - 37s - loss: 0.0165 - val_loss: 0.2630\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.25703\n",
            "Epoch 211/400\n",
            "47/47 - 37s - loss: 0.0169 - val_loss: 0.2594\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.25703\n",
            "Epoch 212/400\n",
            "47/47 - 37s - loss: 0.0163 - val_loss: 0.2604\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.25703\n",
            "Epoch 213/400\n",
            "47/47 - 37s - loss: 0.0163 - val_loss: 0.2605\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.25703\n",
            "Epoch 214/400\n",
            "47/47 - 37s - loss: 0.0154 - val_loss: 0.2615\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.25703\n",
            "Epoch 215/400\n",
            "47/47 - 37s - loss: 0.0156 - val_loss: 0.2614\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.25703\n",
            "Epoch 216/400\n",
            "47/47 - 37s - loss: 0.0158 - val_loss: 0.2614\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.25703\n",
            "Epoch 217/400\n",
            "47/47 - 37s - loss: 0.0159 - val_loss: 0.2631\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.25703\n",
            "Epoch 218/400\n",
            "47/47 - 37s - loss: 0.0175 - val_loss: 0.2669\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.25703\n",
            "Epoch 219/400\n",
            "47/47 - 37s - loss: 0.0196 - val_loss: 0.2696\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.25703\n",
            "Epoch 220/400\n",
            "47/47 - 37s - loss: 0.0257 - val_loss: 0.2708\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.25703\n",
            "Epoch 221/400\n",
            "47/47 - 37s - loss: 0.0287 - val_loss: 0.2729\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.25703\n",
            "Epoch 222/400\n",
            "47/47 - 37s - loss: 0.0305 - val_loss: 0.2722\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.25703\n",
            "Epoch 223/400\n",
            "47/47 - 37s - loss: 0.0323 - val_loss: 0.2749\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.25703\n",
            "Epoch 224/400\n",
            "47/47 - 37s - loss: 0.0343 - val_loss: 0.2723\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.25703\n",
            "Epoch 225/400\n",
            "47/47 - 37s - loss: 0.0323 - val_loss: 0.2712\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.25703\n",
            "Epoch 226/400\n",
            "47/47 - 37s - loss: 0.0285 - val_loss: 0.2770\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.25703\n",
            "Epoch 227/400\n",
            "47/47 - 37s - loss: 0.0243 - val_loss: 0.2716\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.25703\n",
            "Epoch 228/400\n",
            "47/47 - 36s - loss: 0.0206 - val_loss: 0.2675\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.25703\n",
            "Epoch 229/400\n",
            "47/47 - 36s - loss: 0.0186 - val_loss: 0.2658\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.25703\n",
            "Epoch 230/400\n",
            "47/47 - 37s - loss: 0.0170 - val_loss: 0.2653\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.25703\n",
            "Epoch 231/400\n",
            "47/47 - 37s - loss: 0.0151 - val_loss: 0.2640\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.25703\n",
            "Epoch 232/400\n",
            "47/47 - 37s - loss: 0.0146 - val_loss: 0.2611\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.25703\n",
            "Epoch 233/400\n",
            "47/47 - 37s - loss: 0.0138 - val_loss: 0.2634\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.25703\n",
            "Epoch 234/400\n",
            "47/47 - 37s - loss: 0.0134 - val_loss: 0.2604\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.25703\n",
            "Epoch 235/400\n",
            "47/47 - 37s - loss: 0.0128 - val_loss: 0.2627\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.25703\n",
            "Epoch 236/400\n",
            "47/47 - 38s - loss: 0.0134 - val_loss: 0.2631\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.25703\n",
            "Epoch 237/400\n",
            "47/47 - 38s - loss: 0.0150 - val_loss: 0.2645\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.25703\n",
            "Epoch 238/400\n",
            "47/47 - 37s - loss: 0.0149 - val_loss: 0.2635\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.25703\n",
            "Epoch 239/400\n",
            "47/47 - 37s - loss: 0.0144 - val_loss: 0.2643\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.25703\n",
            "Epoch 240/400\n",
            "47/47 - 37s - loss: 0.0137 - val_loss: 0.2629\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.25703\n",
            "Epoch 241/400\n",
            "47/47 - 36s - loss: 0.0115 - val_loss: 0.2626\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.25703\n",
            "Epoch 242/400\n",
            "47/47 - 36s - loss: 0.0111 - val_loss: 0.2664\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.25703\n",
            "Epoch 243/400\n",
            "47/47 - 36s - loss: 0.0111 - val_loss: 0.2612\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.25703\n",
            "Epoch 244/400\n",
            "47/47 - 36s - loss: 0.0111 - val_loss: 0.2603\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.25703\n",
            "Epoch 245/400\n",
            "47/47 - 36s - loss: 0.0105 - val_loss: 0.2628\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.25703\n",
            "Epoch 246/400\n",
            "47/47 - 36s - loss: 0.0096 - val_loss: 0.2604\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.25703\n",
            "Epoch 247/400\n",
            "47/47 - 36s - loss: 0.0093 - val_loss: 0.2618\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.25703\n",
            "Epoch 248/400\n",
            "47/47 - 36s - loss: 0.0092 - val_loss: 0.2630\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.25703\n",
            "Epoch 249/400\n",
            "47/47 - 36s - loss: 0.0098 - val_loss: 0.2632\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.25703\n",
            "Epoch 250/400\n",
            "47/47 - 36s - loss: 0.0106 - val_loss: 0.2622\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.25703\n",
            "Epoch 251/400\n",
            "47/47 - 36s - loss: 0.0110 - val_loss: 0.2625\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.25703\n",
            "Epoch 252/400\n",
            "47/47 - 36s - loss: 0.0107 - val_loss: 0.2638\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.25703\n",
            "Epoch 253/400\n",
            "47/47 - 36s - loss: 0.0111 - val_loss: 0.2668\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.25703\n",
            "Epoch 254/400\n",
            "47/47 - 36s - loss: 0.0130 - val_loss: 0.2665\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.25703\n",
            "Epoch 255/400\n",
            "47/47 - 36s - loss: 0.0177 - val_loss: 0.2771\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.25703\n",
            "Epoch 256/400\n",
            "47/47 - 36s - loss: 0.0220 - val_loss: 0.2718\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.25703\n",
            "Epoch 257/400\n",
            "47/47 - 36s - loss: 0.0290 - val_loss: 0.2815\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.25703\n",
            "Epoch 258/400\n",
            "47/47 - 36s - loss: 0.0384 - val_loss: 0.2865\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.25703\n",
            "Epoch 259/400\n",
            "47/47 - 36s - loss: 0.0462 - val_loss: 0.2873\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.25703\n",
            "Epoch 260/400\n",
            "47/47 - 37s - loss: 0.0471 - val_loss: 0.2871\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.25703\n",
            "Epoch 261/400\n",
            "47/47 - 37s - loss: 0.0432 - val_loss: 0.2829\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.25703\n",
            "Epoch 262/400\n",
            "47/47 - 36s - loss: 0.0336 - val_loss: 0.2759\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.25703\n",
            "Epoch 263/400\n",
            "47/47 - 36s - loss: 0.0240 - val_loss: 0.2686\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.25703\n",
            "Epoch 264/400\n",
            "47/47 - 36s - loss: 0.0183 - val_loss: 0.2672\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.25703\n",
            "Epoch 265/400\n",
            "47/47 - 36s - loss: 0.0141 - val_loss: 0.2654\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.25703\n",
            "Epoch 266/400\n",
            "47/47 - 36s - loss: 0.0123 - val_loss: 0.2645\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.25703\n",
            "Epoch 267/400\n",
            "47/47 - 36s - loss: 0.0102 - val_loss: 0.2631\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.25703\n",
            "Epoch 268/400\n",
            "47/47 - 37s - loss: 0.0090 - val_loss: 0.2618\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.25703\n",
            "Epoch 269/400\n",
            "47/47 - 37s - loss: 0.0085 - val_loss: 0.2624\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.25703\n",
            "Epoch 270/400\n",
            "47/47 - 36s - loss: 0.0080 - val_loss: 0.2618\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.25703\n",
            "Epoch 271/400\n",
            "47/47 - 36s - loss: 0.0079 - val_loss: 0.2641\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.25703\n",
            "Epoch 272/400\n",
            "47/47 - 37s - loss: 0.0082 - val_loss: 0.2629\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.25703\n",
            "Epoch 273/400\n",
            "47/47 - 36s - loss: 0.0085 - val_loss: 0.2633\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.25703\n",
            "Epoch 274/400\n",
            "47/47 - 37s - loss: 0.0085 - val_loss: 0.2629\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.25703\n",
            "Epoch 275/400\n",
            "47/47 - 38s - loss: 0.0081 - val_loss: 0.2628\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.25703\n",
            "Epoch 276/400\n",
            "47/47 - 38s - loss: 0.0082 - val_loss: 0.2650\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.25703\n",
            "Epoch 277/400\n",
            "47/47 - 38s - loss: 0.0097 - val_loss: 0.2662\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.25703\n",
            "Epoch 278/400\n",
            "47/47 - 38s - loss: 0.0120 - val_loss: 0.2655\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.25703\n",
            "Epoch 279/400\n",
            "47/47 - 38s - loss: 0.0113 - val_loss: 0.2655\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.25703\n",
            "Epoch 280/400\n",
            "47/47 - 38s - loss: 0.0104 - val_loss: 0.2643\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.25703\n",
            "Epoch 281/400\n",
            "47/47 - 38s - loss: 0.0096 - val_loss: 0.2660\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.25703\n",
            "Epoch 282/400\n",
            "47/47 - 39s - loss: 0.0087 - val_loss: 0.2639\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.25703\n",
            "Epoch 283/400\n",
            "47/47 - 37s - loss: 0.0088 - val_loss: 0.2678\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.25703\n",
            "Epoch 284/400\n",
            "47/47 - 37s - loss: 0.0099 - val_loss: 0.2653\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.25703\n",
            "Epoch 285/400\n",
            "47/47 - 38s - loss: 0.0102 - val_loss: 0.2649\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.25703\n",
            "Epoch 286/400\n",
            "47/47 - 40s - loss: 0.0098 - val_loss: 0.2665\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.25703\n",
            "Epoch 287/400\n",
            "47/47 - 38s - loss: 0.0093 - val_loss: 0.2651\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.25703\n",
            "Epoch 288/400\n",
            "47/47 - 38s - loss: 0.0096 - val_loss: 0.2659\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.25703\n",
            "Epoch 289/400\n",
            "47/47 - 38s - loss: 0.0114 - val_loss: 0.2711\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.25703\n",
            "Epoch 290/400\n",
            "47/47 - 39s - loss: 0.0111 - val_loss: 0.2739\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.25703\n",
            "Epoch 291/400\n",
            "47/47 - 38s - loss: 0.0148 - val_loss: 0.2757\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.25703\n",
            "Epoch 292/400\n",
            "47/47 - 37s - loss: 0.0213 - val_loss: 0.2812\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.25703\n",
            "Epoch 293/400\n",
            "47/47 - 37s - loss: 0.0239 - val_loss: 0.2837\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.25703\n",
            "Epoch 294/400\n",
            "47/47 - 37s - loss: 0.0286 - val_loss: 0.2841\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.25703\n",
            "Epoch 295/400\n",
            "47/47 - 37s - loss: 0.0288 - val_loss: 0.2789\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.25703\n",
            "Epoch 296/400\n",
            "47/47 - 36s - loss: 0.0267 - val_loss: 0.2810\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.25703\n",
            "Epoch 297/400\n",
            "47/47 - 37s - loss: 0.0229 - val_loss: 0.2732\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.25703\n",
            "Epoch 298/400\n",
            "47/47 - 37s - loss: 0.0173 - val_loss: 0.2696\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.25703\n",
            "Epoch 299/400\n",
            "47/47 - 36s - loss: 0.0141 - val_loss: 0.2680\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.25703\n",
            "Epoch 300/400\n",
            "47/47 - 36s - loss: 0.0112 - val_loss: 0.2684\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.25703\n",
            "Epoch 301/400\n",
            "47/47 - 37s - loss: 0.0093 - val_loss: 0.2695\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.25703\n",
            "Epoch 302/400\n",
            "47/47 - 37s - loss: 0.0084 - val_loss: 0.2646\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.25703\n",
            "Epoch 303/400\n",
            "47/47 - 37s - loss: 0.0076 - val_loss: 0.2646\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.25703\n",
            "Epoch 304/400\n",
            "47/47 - 37s - loss: 0.0070 - val_loss: 0.2658\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.25703\n",
            "Epoch 305/400\n",
            "47/47 - 38s - loss: 0.0068 - val_loss: 0.2660\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.25703\n",
            "Epoch 306/400\n",
            "47/47 - 39s - loss: 0.0070 - val_loss: 0.2668\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.25703\n",
            "Epoch 307/400\n",
            "47/47 - 42s - loss: 0.0071 - val_loss: 0.2670\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.25703\n",
            "Epoch 308/400\n",
            "47/47 - 37s - loss: 0.0071 - val_loss: 0.2655\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.25703\n",
            "Epoch 309/400\n",
            "47/47 - 37s - loss: 0.0071 - val_loss: 0.2675\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.25703\n",
            "Epoch 310/400\n",
            "47/47 - 37s - loss: 0.0066 - val_loss: 0.2666\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.25703\n",
            "Epoch 311/400\n",
            "47/47 - 37s - loss: 0.0063 - val_loss: 0.2674\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.25703\n",
            "Epoch 312/400\n",
            "47/47 - 37s - loss: 0.0064 - val_loss: 0.2656\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.25703\n",
            "Epoch 313/400\n",
            "47/47 - 36s - loss: 0.0065 - val_loss: 0.2668\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.25703\n",
            "Epoch 314/400\n",
            "47/47 - 36s - loss: 0.0065 - val_loss: 0.2674\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.25703\n",
            "Epoch 315/400\n",
            "47/47 - 36s - loss: 0.0066 - val_loss: 0.2664\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.25703\n",
            "Epoch 316/400\n",
            "47/47 - 36s - loss: 0.0072 - val_loss: 0.2681\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.25703\n",
            "Epoch 317/400\n",
            "47/47 - 36s - loss: 0.0081 - val_loss: 0.2699\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.25703\n",
            "Epoch 318/400\n",
            "47/47 - 36s - loss: 0.0090 - val_loss: 0.2712\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.25703\n",
            "Epoch 319/400\n",
            "47/47 - 36s - loss: 0.0096 - val_loss: 0.2703\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.25703\n",
            "Epoch 320/400\n",
            "47/47 - 36s - loss: 0.0110 - val_loss: 0.2741\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.25703\n",
            "Epoch 321/400\n",
            "47/47 - 36s - loss: 0.0143 - val_loss: 0.2774\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.25703\n",
            "Epoch 322/400\n",
            "47/47 - 37s - loss: 0.0154 - val_loss: 0.2753\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.25703\n",
            "Epoch 323/400\n",
            "47/47 - 37s - loss: 0.0173 - val_loss: 0.2736\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.25703\n",
            "Epoch 324/400\n",
            "47/47 - 37s - loss: 0.0170 - val_loss: 0.2761\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.25703\n",
            "Epoch 325/400\n",
            "47/47 - 38s - loss: 0.0190 - val_loss: 0.2738\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.25703\n",
            "Epoch 326/400\n",
            "47/47 - 37s - loss: 0.0161 - val_loss: 0.2783\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.25703\n",
            "Epoch 327/400\n",
            "47/47 - 37s - loss: 0.0183 - val_loss: 0.2755\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.25703\n",
            "Epoch 328/400\n",
            "47/47 - 37s - loss: 0.0220 - val_loss: 0.2762\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.25703\n",
            "Epoch 329/400\n",
            "47/47 - 37s - loss: 0.0223 - val_loss: 0.2807\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.25703\n",
            "Epoch 330/400\n",
            "47/47 - 36s - loss: 0.0205 - val_loss: 0.2806\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.25703\n",
            "Epoch 331/400\n",
            "47/47 - 37s - loss: 0.0183 - val_loss: 0.2828\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.25703\n",
            "Epoch 332/400\n",
            "47/47 - 37s - loss: 0.0192 - val_loss: 0.2788\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.25703\n",
            "Epoch 333/400\n",
            "47/47 - 36s - loss: 0.0193 - val_loss: 0.2770\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.25703\n",
            "Epoch 334/400\n",
            "47/47 - 37s - loss: 0.0156 - val_loss: 0.2740\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.25703\n",
            "Epoch 335/400\n",
            "47/47 - 38s - loss: 0.0140 - val_loss: 0.2740\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.25703\n",
            "Epoch 336/400\n",
            "47/47 - 44s - loss: 0.0130 - val_loss: 0.2713\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.25703\n",
            "Epoch 337/400\n",
            "47/47 - 41s - loss: 0.0102 - val_loss: 0.2735\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.25703\n",
            "Epoch 338/400\n",
            "47/47 - 38s - loss: 0.0088 - val_loss: 0.2697\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.25703\n",
            "Epoch 339/400\n",
            "47/47 - 37s - loss: 0.0075 - val_loss: 0.2693\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.25703\n",
            "Epoch 340/400\n",
            "47/47 - 38s - loss: 0.0071 - val_loss: 0.2679\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.25703\n",
            "Epoch 341/400\n",
            "47/47 - 42s - loss: 0.0065 - val_loss: 0.2692\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.25703\n",
            "Epoch 342/400\n",
            "47/47 - 39s - loss: 0.0063 - val_loss: 0.2691\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.25703\n",
            "Epoch 343/400\n",
            "47/47 - 37s - loss: 0.0064 - val_loss: 0.2706\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.25703\n",
            "Epoch 344/400\n",
            "47/47 - 37s - loss: 0.0068 - val_loss: 0.2684\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.25703\n",
            "Epoch 345/400\n",
            "47/47 - 37s - loss: 0.0065 - val_loss: 0.2690\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.25703\n",
            "Epoch 346/400\n",
            "47/47 - 37s - loss: 0.0062 - val_loss: 0.2688\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.25703\n",
            "Epoch 347/400\n",
            "47/47 - 39s - loss: 0.0061 - val_loss: 0.2692\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.25703\n",
            "Epoch 348/400\n",
            "47/47 - 39s - loss: 0.0059 - val_loss: 0.2698\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.25703\n",
            "Epoch 349/400\n",
            "47/47 - 44s - loss: 0.0060 - val_loss: 0.2691\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.25703\n",
            "Epoch 350/400\n",
            "47/47 - 43s - loss: 0.0064 - val_loss: 0.2689\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.25703\n",
            "Epoch 351/400\n",
            "47/47 - 42s - loss: 0.0069 - val_loss: 0.2712\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.25703\n",
            "Epoch 352/400\n",
            "47/47 - 40s - loss: 0.0087 - val_loss: 0.2706\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.25703\n",
            "Epoch 353/400\n",
            "47/47 - 40s - loss: 0.0101 - val_loss: 0.2729\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.25703\n",
            "Epoch 354/400\n",
            "47/47 - 42s - loss: 0.0087 - val_loss: 0.2714\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.25703\n",
            "Epoch 355/400\n",
            "47/47 - 44s - loss: 0.0082 - val_loss: 0.2683\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.25703\n",
            "Epoch 356/400\n",
            "47/47 - 41s - loss: 0.0081 - val_loss: 0.2715\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.25703\n",
            "Epoch 357/400\n",
            "47/47 - 40s - loss: 0.0080 - val_loss: 0.2680\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.25703\n",
            "Epoch 358/400\n",
            "47/47 - 40s - loss: 0.0074 - val_loss: 0.2698\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.25703\n",
            "Epoch 359/400\n",
            "47/47 - 42s - loss: 0.0076 - val_loss: 0.2704\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.25703\n",
            "Epoch 360/400\n",
            "47/47 - 43s - loss: 0.0075 - val_loss: 0.2691\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.25703\n",
            "Epoch 361/400\n",
            "47/47 - 40s - loss: 0.0071 - val_loss: 0.2699\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.25703\n",
            "Epoch 362/400\n",
            "47/47 - 43s - loss: 0.0067 - val_loss: 0.2688\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.25703\n",
            "Epoch 363/400\n",
            "47/47 - 42s - loss: 0.0069 - val_loss: 0.2706\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.25703\n",
            "Epoch 364/400\n",
            "47/47 - 40s - loss: 0.0070 - val_loss: 0.2700\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.25703\n",
            "Epoch 365/400\n",
            "47/47 - 39s - loss: 0.0075 - val_loss: 0.2696\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.25703\n",
            "Epoch 366/400\n",
            "47/47 - 40s - loss: 0.0097 - val_loss: 0.2744\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.25703\n",
            "Epoch 367/400\n",
            "47/47 - 40s - loss: 0.0126 - val_loss: 0.2830\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.25703\n",
            "Epoch 368/400\n",
            "47/47 - 46s - loss: 0.0177 - val_loss: 0.2833\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.25703\n",
            "Epoch 369/400\n",
            "47/47 - 46s - loss: 0.0262 - val_loss: 0.2902\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.25703\n",
            "Epoch 370/400\n",
            "47/47 - 41s - loss: 0.0343 - val_loss: 0.2959\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.25703\n",
            "Epoch 371/400\n",
            "47/47 - 42s - loss: 0.0454 - val_loss: 0.2952\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.25703\n",
            "Epoch 372/400\n",
            "47/47 - 43s - loss: 0.0397 - val_loss: 0.2873\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.25703\n",
            "Epoch 373/400\n",
            "47/47 - 40s - loss: 0.0323 - val_loss: 0.2822\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.25703\n",
            "Epoch 374/400\n",
            "47/47 - 37s - loss: 0.0264 - val_loss: 0.2796\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.25703\n",
            "Epoch 375/400\n",
            "47/47 - 37s - loss: 0.0176 - val_loss: 0.2729\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.25703\n",
            "Epoch 376/400\n",
            "47/47 - 37s - loss: 0.0122 - val_loss: 0.2754\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.25703\n",
            "Epoch 377/400\n",
            "47/47 - 37s - loss: 0.0097 - val_loss: 0.2701\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.25703\n",
            "Epoch 378/400\n",
            "47/47 - 37s - loss: 0.0080 - val_loss: 0.2682\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.25703\n",
            "Epoch 379/400\n",
            "47/47 - 37s - loss: 0.0070 - val_loss: 0.2679\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.25703\n",
            "Epoch 380/400\n",
            "47/47 - 38s - loss: 0.0064 - val_loss: 0.2684\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.25703\n",
            "Epoch 381/400\n",
            "47/47 - 39s - loss: 0.0062 - val_loss: 0.2673\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.25703\n",
            "Epoch 382/400\n",
            "47/47 - 46s - loss: 0.0061 - val_loss: 0.2681\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.25703\n",
            "Epoch 383/400\n",
            "47/47 - 41s - loss: 0.0057 - val_loss: 0.2684\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.25703\n",
            "Epoch 384/400\n",
            "47/47 - 39s - loss: 0.0057 - val_loss: 0.2683\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.25703\n",
            "Epoch 385/400\n",
            "47/47 - 38s - loss: 0.0056 - val_loss: 0.2687\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.25703\n",
            "Epoch 386/400\n",
            "47/47 - 40s - loss: 0.0056 - val_loss: 0.2686\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.25703\n",
            "Epoch 387/400\n",
            "47/47 - 39s - loss: 0.0056 - val_loss: 0.2683\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.25703\n",
            "Epoch 388/400\n",
            "47/47 - 38s - loss: 0.0058 - val_loss: 0.2693\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.25703\n",
            "Epoch 389/400\n",
            "47/47 - 37s - loss: 0.0059 - val_loss: 0.2688\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.25703\n",
            "Epoch 390/400\n",
            "47/47 - 37s - loss: 0.0059 - val_loss: 0.2703\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.25703\n",
            "Epoch 391/400\n",
            "47/47 - 37s - loss: 0.0059 - val_loss: 0.2700\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.25703\n",
            "Epoch 392/400\n",
            "47/47 - 37s - loss: 0.0057 - val_loss: 0.2700\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.25703\n",
            "Epoch 393/400\n",
            "47/47 - 37s - loss: 0.0055 - val_loss: 0.2698\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.25703\n",
            "Epoch 394/400\n",
            "47/47 - 37s - loss: 0.0055 - val_loss: 0.2693\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.25703\n",
            "Epoch 395/400\n",
            "47/47 - 38s - loss: 0.0053 - val_loss: 0.2702\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.25703\n",
            "Epoch 396/400\n",
            "47/47 - 38s - loss: 0.0055 - val_loss: 0.2697\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.25703\n",
            "Epoch 397/400\n",
            "47/47 - 37s - loss: 0.0061 - val_loss: 0.2700\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.25703\n",
            "Epoch 398/400\n",
            "47/47 - 37s - loss: 0.0058 - val_loss: 0.2710\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.25703\n",
            "Epoch 399/400\n",
            "47/47 - 37s - loss: 0.0060 - val_loss: 0.2702\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.25703\n",
            "Epoch 400/400\n",
            "47/47 - 37s - loss: 0.0067 - val_loss: 0.2711\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.25703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb737335250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ9tc_AG1U3u",
        "outputId": "7e3fa6c5-3678-4210-f611-67c3a3076f05"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Cantonese-both.pkl')\n",
        "train = load_clean_sentences('english-Cantonese-train.pkl')\n",
        "test = load_clean_sentences('english-Cantonese-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[呢塊布摸上手好滑。], target=[This cloth feels smooth], predicted=[this cloth feels smooth]\n",
            "src=[我哋而家做緊嘅嘢好危險。], target=[What were doing now is very dangerous], predicted=[either doing doing is is very dangerous]\n",
            "src=[全年最正就係呢個時候。], target=[This is the best time of year], predicted=[this is the best time of year]\n",
            "src=[佢俾啲細路每人兩個蘋果。], target=[She gave the children two apples each], predicted=[she gave the children two apples each]\n",
            "src=[班小朋友瞓緊覺㗎。唔該你細聲啲啦。], target=[The children are sleeping Please dont be so noisy], predicted=[the children not sleeping away dont be so noisy]\n",
            "src=[有啲人話世界上係冇真愛嘅。], target=[Some people say true love doesnt exist], predicted=[some people say that true doesnt doesnt exist]\n",
            "src=[我嗰班嗰啲女仔全部都好好人。], target=[All the girls in my class are kind], predicted=[at the parents the the class are]\n",
            "src=[佢去到個火車站嗰陣，班火車已經走咗差唔多半粒鐘。], target=[When he reached the station the train had already left almost half an hour before], predicted=[i train reached the the the train train left left almost half half before before]\n",
            "src=[我整喊咗阿Tom。], target=[I made Tom cry], predicted=[i i tom cry]\n",
            "src=[Tom叫Mary去驗吓眼。], target=[Tom told Mary she should get her eyes checked], predicted=[tom me that that should get her eyes checked]\n",
            "BLEU-1: 0.658084\n",
            "BLEU-2: 0.560846\n",
            "BLEU-3: 0.511603\n",
            "BLEU-4: 0.398400\n",
            "test\n",
            "src=[五加二喺七。], target=[Five plus two equals seven], predicted=[five plus two equals seven]\n",
            "src=[呢間酒店裝得落一千人。], target=[This hotel has accommodations for 1000 guests], predicted=[this hotel has accommodations for 1000 guests]\n",
            "src=[佢舊年開始喺嗰間公司度返工。], target=[He began to work for that company last year], predicted=[he began to for for for company last year]\n",
            "src=[我唔明點解我可以犯埋啲咁嘅錯。], target=[I cant conceive how I could have made such a mistake], predicted=[i cant conceive whether could could have have in a mistake]\n",
            "src=[佢開美容院。], target=[She runs a beauty shop], predicted=[she runs a beauty shop]\n",
            "src=[我鍾意睇書。], target=[I like reading books], predicted=[i like reading books]\n",
            "src=[你知嘅嘢我哋都知。], target=[We know everything you know], predicted=[we know everything you know]\n",
            "src=[我係網球俱樂部嘅會員。], target=[I am a member of the tennis club], predicted=[i am a member of the tennis club]\n",
            "src=[啲屋一間一間噉著咗火。], target=[The houses caught fire one after another], predicted=[the houses caught fire one after another]\n",
            "src=[老狗唔可以教新招。], target=[You cant teach an old dog new tricks], predicted=[you cant teach that dog dog new tricks]\n",
            "BLEU-1: 0.604624\n",
            "BLEU-2: 0.513981\n",
            "BLEU-3: 0.472468\n",
            "BLEU-4: 0.364360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJvFhlNAIIZ0",
        "outputId": "401ba039-6c27-4764-a5ff-62bf7858b05b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Cantonese-both.pkl')\n",
        "train = load_clean_sentences('english-Cantonese-train.pkl')\n",
        "test = load_clean_sentences('english-Cantonese-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[我冇錢，但係我有夢想。], target=[I have no money but I have dreams], predicted=[i have no money but i have dreams]\n",
            "src=[佢仲要問佢阿爸阿媽攞錢。], target=[She is still financially dependent on her parents], predicted=[she is still financially dependent on her parents]\n",
            "src=[你會唔會覺得我份人太過物質呀？], target=[Do you think Im too materialistic], predicted=[do you think im too materialistic]\n",
            "src=[我想學跳舞。], target=[I would like to learn how to dance], predicted=[i would like to learn how to dance]\n",
            "src=[我哋要請一個新嘅經理。], target=[We need to hire a new manager], predicted=[we need to hire a new manager]\n",
            "src=[你而家係大人嚟㗎喇，唔好再好似成個細路咁喇。], target=[Now that you are grown up you must not behave like a child], predicted=[now that you are grown up you must not behave like a child]\n",
            "src=[試吓從我個角度去諗吖。], target=[Try to look at it from my point of view], predicted=[try to look at it from my point of view]\n",
            "src=[Tom係我哥哥。], target=[Tom is my older brother], predicted=[tom is my older brother]\n",
            "src=[我等咗我個friend成粒鐘。], target=[Ive spent an entire hour waiting for my friend], predicted=[ive spent an entire hour waiting for my friend]\n",
            "src=[你咁嘅成績入唔到大學㗎喎。], target=[With those results you wont be able to go to university], predicted=[with those results you wont be able to go to university]\n",
            "BLEU-1: 0.818451\n",
            "BLEU-2: 0.787867\n",
            "BLEU-3: 0.776278\n",
            "BLEU-4: 0.718275\n",
            "test\n",
            "src=[佢哋受過訓練。], target=[Theyve been trained], predicted=[theyve been trained]\n",
            "src=[啲朱古力蛋糕隨便食喎。], target=[Please help yourself to the chocolate cake], predicted=[please help yourself to the chocolate cake]\n",
            "src=[你介唔介意講大聲少少呀？], target=[Would you mind speaking a little louder], predicted=[would you mind speaking a little louder]\n",
            "src=[實驗會唔會成功呢？], target=[Will the experiment succeed], predicted=[will the experiment succeed]\n",
            "src=[我哋食完晚飯，不如去海灘散步吖。], target=[Lets walk on the beach after dinner], predicted=[lets walk on the beach after dinner]\n",
            "src=[我又結咗婚喇。], target=[I got married again], predicted=[i got married again]\n",
            "src=[我個仔仲未識計加數。], target=[My boy cant do addition properly yet], predicted=[my boy cant do addition properly yet]\n",
            "src=[佢返到嚟嗰陣，我啱啱好寫完封信。], target=[I had just written the letter when he came back], predicted=[i had just written the letter when he came back]\n",
            "src=[聖誕節絕對係我最鍾意嘅節日。], target=[Christmas is definitely my favorite holiday], predicted=[christmas is definitely my favorite holiday]\n",
            "src=[佢上個禮拜病咗。], target=[He was sick last week], predicted=[he was sick last week]\n",
            "BLEU-1: 0.747087\n",
            "BLEU-2: 0.718129\n",
            "BLEU-3: 0.713691\n",
            "BLEU-4: 0.653991\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}